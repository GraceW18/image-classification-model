{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GraceW18/image-classification-model-CNN/blob/main/RandomizedSearchCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XwEEQBHDddLn",
        "outputId": "721d06aa-ae2e-458b-bdcb-8d239c6954fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
            "Training data shape: (130, 28, 28, 1)\n",
            "Test data shape: (130, 28, 28, 1)\n",
            "Number of classes: 10\n",
            "\n",
            "Generating random parameter combinations...\n",
            "Starting Randomized Parameter Search...\n",
            "============================================================\n",
            "\n",
            "Combination 1/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 32\n",
            "  dropout_rate: 0.2\n",
            "  dense_units: 32\n",
            "  learning_rate: 0.01\n",
            "  optimizer: rmsprop\n",
            "  epochs: 30\n",
            "  batch_size: 8\n",
            "    Fold 1/3     Building model: filters=32, dropout=0.2, units=32, lr=0.01, opt=rmsprop\n",
            "Score: 0.091\n",
            "    Fold 2/3     Building model: filters=32, dropout=0.2, units=32, lr=0.01, opt=rmsprop\n",
            "Score: 0.814\n",
            "    Fold 3/3     Building model: filters=32, dropout=0.2, units=32, lr=0.01, opt=rmsprop\n",
            "Score: 0.116\n",
            "  CV Result: 0.3404 (+/- 0.6701)\n",
            "  Time: 32.6s\n",
            "  *** NEW BEST SCORE! ***\n",
            "\n",
            "Combination 2/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 16\n",
            "  dropout_rate: 0.2\n",
            "  dense_units: 128\n",
            "  learning_rate: 0.01\n",
            "  optimizer: adam\n",
            "  epochs: 80\n",
            "  batch_size: 16\n",
            "    Fold 1/3     Building model: filters=16, dropout=0.2, units=128, lr=0.01, opt=adam\n",
            "Score: 0.886\n",
            "    Fold 2/3     Building model: filters=16, dropout=0.2, units=128, lr=0.01, opt=adam\n",
            "Score: 0.791\n",
            "    Fold 3/3     Building model: filters=16, dropout=0.2, units=128, lr=0.01, opt=adam\n",
            "Score: 0.930\n",
            "  CV Result: 0.8691 (+/- 0.1165)\n",
            "  Time: 60.4s\n",
            "  *** NEW BEST SCORE! ***\n",
            "\n",
            "Combination 3/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 16\n",
            "  dropout_rate: 0.2\n",
            "  dense_units: 32\n",
            "  learning_rate: 0.001\n",
            "  optimizer: adam\n",
            "  epochs: 80\n",
            "  batch_size: 32\n",
            "    Fold 1/3     Building model: filters=16, dropout=0.2, units=32, lr=0.001, opt=adam\n",
            "Score: 0.795\n",
            "    Fold 2/3     Building model: filters=16, dropout=0.2, units=32, lr=0.001, opt=adam\n",
            "Score: 0.837\n",
            "    Fold 3/3     Building model: filters=16, dropout=0.2, units=32, lr=0.001, opt=adam\n",
            "Score: 0.884\n",
            "  CV Result: 0.8388 (+/- 0.0721)\n",
            "  Time: 53.0s\n",
            "\n",
            "Combination 4/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 16\n",
            "  dropout_rate: 0.3\n",
            "  dense_units: 128\n",
            "  learning_rate: 0.01\n",
            "  optimizer: rmsprop\n",
            "  epochs: 30\n",
            "  batch_size: 16\n",
            "    Fold 1/3     Building model: filters=16, dropout=0.3, units=128, lr=0.01, opt=rmsprop\n",
            "Score: 0.818\n",
            "    Fold 2/3     Building model: filters=16, dropout=0.3, units=128, lr=0.01, opt=rmsprop\n",
            "Score: 0.744\n",
            "    Fold 3/3     Building model: filters=16, dropout=0.3, units=128, lr=0.01, opt=rmsprop\n",
            "Score: 0.744\n",
            "  CV Result: 0.7689 (+/- 0.0698)\n",
            "  Time: 24.9s\n",
            "\n",
            "Combination 5/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 32\n",
            "  dropout_rate: 0.4\n",
            "  dense_units: 32\n",
            "  learning_rate: 0.001\n",
            "  optimizer: rmsprop\n",
            "  epochs: 50\n",
            "  batch_size: 16\n",
            "    Fold 1/3     Building model: filters=32, dropout=0.4, units=32, lr=0.001, opt=rmsprop\n",
            "Score: 0.773\n",
            "    Fold 2/3     Building model: filters=32, dropout=0.4, units=32, lr=0.001, opt=rmsprop\n",
            "Score: 0.767\n",
            "    Fold 3/3     Building model: filters=32, dropout=0.4, units=32, lr=0.001, opt=rmsprop\n",
            "Score: 0.767\n",
            "  CV Result: 0.7692 (+/- 0.0050)\n",
            "  Time: 49.1s\n",
            "\n",
            "Combination 6/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 16\n",
            "  dropout_rate: 0.3\n",
            "  dense_units: 64\n",
            "  learning_rate: 0.001\n",
            "  optimizer: adam\n",
            "  epochs: 50\n",
            "  batch_size: 8\n",
            "    Fold 1/3     Building model: filters=16, dropout=0.3, units=64, lr=0.001, opt=adam\n",
            "Score: 0.795\n",
            "    Fold 2/3     Building model: filters=16, dropout=0.3, units=64, lr=0.001, opt=adam\n",
            "Score: 0.814\n",
            "    Fold 3/3     Building model: filters=16, dropout=0.3, units=64, lr=0.001, opt=adam\n",
            "Score: 0.884\n",
            "  CV Result: 0.8310 (+/- 0.0760)\n",
            "  Time: 48.8s\n",
            "\n",
            "Combination 7/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 24\n",
            "  dropout_rate: 0.4\n",
            "  dense_units: 128\n",
            "  learning_rate: 0.005\n",
            "  optimizer: adam\n",
            "  epochs: 80\n",
            "  batch_size: 16\n",
            "    Fold 1/3     Building model: filters=24, dropout=0.4, units=128, lr=0.005, opt=adam\n",
            "Score: 0.909\n",
            "    Fold 2/3     Building model: filters=24, dropout=0.4, units=128, lr=0.005, opt=adam\n",
            "Score: 0.860\n",
            "    Fold 3/3     Building model: filters=24, dropout=0.4, units=128, lr=0.005, opt=adam\n",
            "Score: 0.953\n",
            "  CV Result: 0.9077 (+/- 0.0760)\n",
            "  Time: 69.1s\n",
            "  *** NEW BEST SCORE! ***\n",
            "\n",
            "Combination 8/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 32\n",
            "  dropout_rate: 0.2\n",
            "  dense_units: 64\n",
            "  learning_rate: 0.001\n",
            "  optimizer: rmsprop\n",
            "  epochs: 80\n",
            "  batch_size: 32\n",
            "    Fold 1/3     Building model: filters=32, dropout=0.2, units=64, lr=0.001, opt=rmsprop\n",
            "Score: 0.864\n",
            "    Fold 2/3     Building model: filters=32, dropout=0.2, units=64, lr=0.001, opt=rmsprop\n",
            "Score: 0.907\n",
            "    Fold 3/3     Building model: filters=32, dropout=0.2, units=64, lr=0.001, opt=rmsprop\n",
            "Score: 0.860\n",
            "  CV Result: 0.8770 (+/- 0.0424)\n",
            "  Time: 65.5s\n",
            "\n",
            "Combination 9/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 24\n",
            "  dropout_rate: 0.3\n",
            "  dense_units: 128\n",
            "  learning_rate: 0.001\n",
            "  optimizer: adam\n",
            "  epochs: 80\n",
            "  batch_size: 8\n",
            "    Fold 1/3     Building model: filters=24, dropout=0.3, units=128, lr=0.001, opt=adam\n",
            "Score: 0.886\n",
            "    Fold 2/3     Building model: filters=24, dropout=0.3, units=128, lr=0.001, opt=adam\n",
            "Score: 0.884\n",
            "    Fold 3/3     Building model: filters=24, dropout=0.3, units=128, lr=0.001, opt=adam\n",
            "Score: 0.884\n",
            "  CV Result: 0.8846 (+/- 0.0025)\n",
            "  Time: 77.2s\n",
            "\n",
            "Combination 10/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 24\n",
            "  dropout_rate: 0.2\n",
            "  dense_units: 32\n",
            "  learning_rate: 0.001\n",
            "  optimizer: rmsprop\n",
            "  epochs: 50\n",
            "  batch_size: 16\n",
            "    Fold 1/3     Building model: filters=24, dropout=0.2, units=32, lr=0.001, opt=rmsprop\n",
            "Score: 0.864\n",
            "    Fold 2/3     Building model: filters=24, dropout=0.2, units=32, lr=0.001, opt=rmsprop\n",
            "Score: 0.907\n",
            "    Fold 3/3     Building model: filters=24, dropout=0.2, units=32, lr=0.001, opt=rmsprop\n",
            "Score: 0.744\n",
            "  CV Result: 0.8383 (+/- 0.1377)\n",
            "  Time: 44.8s\n",
            "\n",
            "Combination 11/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 32\n",
            "  dropout_rate: 0.4\n",
            "  dense_units: 32\n",
            "  learning_rate: 0.005\n",
            "  optimizer: rmsprop\n",
            "  epochs: 30\n",
            "  batch_size: 32\n",
            "    Fold 1/3     Building model: filters=32, dropout=0.4, units=32, lr=0.005, opt=rmsprop\n",
            "Score: 0.636\n",
            "    Fold 2/3     Building model: filters=32, dropout=0.4, units=32, lr=0.005, opt=rmsprop\n",
            "Score: 0.698\n",
            "    Fold 3/3     Building model: filters=32, dropout=0.4, units=32, lr=0.005, opt=rmsprop\n",
            "Score: 0.186\n",
            "  CV Result: 0.5067 (+/- 0.4562)\n",
            "  Time: 30.3s\n",
            "\n",
            "Combination 12/12\n",
            "--------------------------------------------------\n",
            "Parameters:\n",
            "  filters: 24\n",
            "  dropout_rate: 0.2\n",
            "  dense_units: 128\n",
            "  learning_rate: 0.01\n",
            "  optimizer: adam\n",
            "  epochs: 80\n",
            "  batch_size: 32\n",
            "    Fold 1/3     Building model: filters=24, dropout=0.2, units=128, lr=0.01, opt=adam\n",
            "Score: 0.864\n",
            "    Fold 2/3     Building model: filters=24, dropout=0.2, units=128, lr=0.01, opt=adam\n",
            "Score: 0.767\n",
            "    Fold 3/3     Building model: filters=24, dropout=0.2, units=128, lr=0.01, opt=adam\n",
            "Score: 0.837\n",
            "  CV Result: 0.8228 (+/- 0.0812)\n",
            "  Time: 60.3s\n",
            "\n",
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "Best CV score: 0.9077\n",
            "Best parameters:\n",
            "  filters: 24\n",
            "  dropout_rate: 0.4\n",
            "  dense_units: 128\n",
            "  learning_rate: 0.005\n",
            "  optimizer: adam\n",
            "  epochs: 80\n",
            "  batch_size: 16\n",
            "\n",
            "----------------------------------------\n",
            "Training final model on full training set...\n",
            "    Building model: filters=24, dropout=0.4, units=128, lr=0.005, opt=adam\n",
            "Epoch 1/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.1058 - loss: 2.3786 - val_accuracy: 0.0000e+00 - val_loss: 2.3196\n",
            "Epoch 2/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1001 - loss: 2.3048 - val_accuracy: 0.0000e+00 - val_loss: 2.3346\n",
            "Epoch 3/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1408 - loss: 2.3001 - val_accuracy: 0.0000e+00 - val_loss: 2.3212\n",
            "Epoch 4/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.1728 - loss: 2.2865 - val_accuracy: 0.0000e+00 - val_loss: 2.3604\n",
            "Epoch 5/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2049 - loss: 2.2408 - val_accuracy: 0.0500 - val_loss: 2.3259\n",
            "Epoch 6/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.2646 - loss: 2.1502 - val_accuracy: 0.0500 - val_loss: 2.1969\n",
            "Epoch 7/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3283 - loss: 1.9301 - val_accuracy: 0.2500 - val_loss: 1.9117\n",
            "Epoch 8/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3936 - loss: 1.6399 - val_accuracy: 0.4500 - val_loss: 1.5518\n",
            "Epoch 9/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5151 - loss: 1.3190 - val_accuracy: 0.5500 - val_loss: 1.2728\n",
            "Epoch 10/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6433 - loss: 1.1418 - val_accuracy: 0.6000 - val_loss: 1.1237\n",
            "Epoch 11/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6301 - loss: 0.9414 - val_accuracy: 0.6000 - val_loss: 0.9682\n",
            "Epoch 12/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7221 - loss: 0.8234 - val_accuracy: 0.7500 - val_loss: 0.8993\n",
            "Epoch 13/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8093 - loss: 0.6302 - val_accuracy: 0.7000 - val_loss: 0.8940\n",
            "Epoch 14/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8048 - loss: 0.4837 - val_accuracy: 0.6000 - val_loss: 0.9152\n",
            "Epoch 15/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9036 - loss: 0.3611 - val_accuracy: 0.7000 - val_loss: 0.8549\n",
            "Epoch 16/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8074 - loss: 0.5037 - val_accuracy: 0.6500 - val_loss: 0.9180\n",
            "Epoch 17/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8912 - loss: 0.2765 - val_accuracy: 0.7000 - val_loss: 1.0330\n",
            "Epoch 18/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8918 - loss: 0.2897 - val_accuracy: 0.7000 - val_loss: 1.0197\n",
            "Epoch 19/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9727 - loss: 0.1234 - val_accuracy: 0.7000 - val_loss: 1.0670\n",
            "Epoch 20/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9839 - loss: 0.1169 - val_accuracy: 0.7500 - val_loss: 1.0269\n",
            "Epoch 21/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9628 - loss: 0.1613 - val_accuracy: 0.7000 - val_loss: 1.1906\n",
            "Epoch 22/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9336 - loss: 0.1645 - val_accuracy: 0.7500 - val_loss: 1.1562\n",
            "Epoch 23/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9725 - loss: 0.1239 - val_accuracy: 0.6500 - val_loss: 1.0817\n",
            "Epoch 24/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9625 - loss: 0.2209 - val_accuracy: 0.7000 - val_loss: 1.1761\n",
            "Epoch 25/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9641 - loss: 0.1060 - val_accuracy: 0.7500 - val_loss: 1.2308\n",
            "Epoch 26/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9532 - loss: 0.1458 - val_accuracy: 0.7000 - val_loss: 1.2442\n",
            "Epoch 27/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9661 - loss: 0.1168 - val_accuracy: 0.7500 - val_loss: 1.2337\n",
            "Epoch 28/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9395 - loss: 0.1293 - val_accuracy: 0.8000 - val_loss: 1.0022\n",
            "Epoch 29/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9855 - loss: 0.0694 - val_accuracy: 0.7000 - val_loss: 1.2602\n",
            "Epoch 30/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9294 - loss: 0.1567 - val_accuracy: 0.7000 - val_loss: 1.4573\n",
            "Epoch 31/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9790 - loss: 0.0901 - val_accuracy: 0.7000 - val_loss: 1.5899\n",
            "Epoch 32/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9841 - loss: 0.0813 - val_accuracy: 0.7000 - val_loss: 1.5437\n",
            "Epoch 33/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9680 - loss: 0.0815 - val_accuracy: 0.6500 - val_loss: 1.1440\n",
            "Epoch 34/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9767 - loss: 0.0787 - val_accuracy: 0.7500 - val_loss: 1.1222\n",
            "Epoch 35/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0392 - val_accuracy: 0.6500 - val_loss: 1.5272\n",
            "Epoch 36/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9977 - loss: 0.0461 - val_accuracy: 0.7000 - val_loss: 1.4119\n",
            "Epoch 37/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9677 - loss: 0.0766 - val_accuracy: 0.6500 - val_loss: 1.1990\n",
            "Epoch 38/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9306 - loss: 0.1325 - val_accuracy: 0.6500 - val_loss: 1.2865\n",
            "Epoch 39/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9878 - loss: 0.0446 - val_accuracy: 0.7000 - val_loss: 1.4429\n",
            "Epoch 40/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9919 - loss: 0.0378 - val_accuracy: 0.7000 - val_loss: 1.5380\n",
            "Epoch 41/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0253 - val_accuracy: 0.6000 - val_loss: 1.7974\n",
            "Epoch 42/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9942 - loss: 0.0358 - val_accuracy: 0.6000 - val_loss: 1.8295\n",
            "Epoch 43/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9786 - loss: 0.0640 - val_accuracy: 0.6500 - val_loss: 1.5854\n",
            "Epoch 44/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9942 - loss: 0.0378 - val_accuracy: 0.7000 - val_loss: 1.5360\n",
            "Epoch 45/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9864 - loss: 0.0224 - val_accuracy: 0.7000 - val_loss: 1.7210\n",
            "Epoch 46/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9864 - loss: 0.0468 - val_accuracy: 0.6500 - val_loss: 1.8661\n",
            "Epoch 47/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9929 - loss: 0.0495 - val_accuracy: 0.7000 - val_loss: 1.7617\n",
            "Epoch 48/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0193 - val_accuracy: 0.7000 - val_loss: 1.8195\n",
            "Epoch 49/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9696 - loss: 0.0703 - val_accuracy: 0.7000 - val_loss: 2.0137\n",
            "Epoch 50/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9884 - loss: 0.0384 - val_accuracy: 0.7000 - val_loss: 1.8802\n",
            "Epoch 51/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9929 - loss: 0.0203 - val_accuracy: 0.7000 - val_loss: 1.8096\n",
            "Epoch 52/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9858 - loss: 0.0874 - val_accuracy: 0.7500 - val_loss: 1.5128\n",
            "Epoch 53/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9767 - loss: 0.0542 - val_accuracy: 0.6000 - val_loss: 1.6570\n",
            "Epoch 54/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9929 - loss: 0.0427 - val_accuracy: 0.7000 - val_loss: 1.5834\n",
            "Epoch 55/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0124 - val_accuracy: 0.7500 - val_loss: 1.5943\n",
            "Epoch 56/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0340 - val_accuracy: 0.7500 - val_loss: 1.5607\n",
            "Epoch 57/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0155 - val_accuracy: 0.7500 - val_loss: 1.7062\n",
            "Epoch 58/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.7500 - val_loss: 1.7188\n",
            "Epoch 59/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9903 - loss: 0.0233 - val_accuracy: 0.7000 - val_loss: 1.7920\n",
            "Epoch 60/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 0.7500 - val_loss: 1.9197\n",
            "Epoch 61/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 0.7500 - val_loss: 1.8593\n",
            "Epoch 62/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.7000 - val_loss: 1.8428\n",
            "Epoch 63/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 0.7000 - val_loss: 1.9200\n",
            "Epoch 64/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9949 - loss: 0.0262 - val_accuracy: 0.7000 - val_loss: 2.0017\n",
            "Epoch 65/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.7000 - val_loss: 1.9784\n",
            "Epoch 66/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9906 - loss: 0.0384 - val_accuracy: 0.7000 - val_loss: 1.9199\n",
            "Epoch 67/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9841 - loss: 0.0255 - val_accuracy: 0.7000 - val_loss: 2.2823\n",
            "Epoch 68/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.7500 - val_loss: 2.4194\n",
            "Epoch 69/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9864 - loss: 0.0270 - val_accuracy: 0.7000 - val_loss: 2.2136\n",
            "Epoch 70/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9715 - loss: 0.1065 - val_accuracy: 0.7000 - val_loss: 2.2637\n",
            "Epoch 71/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9864 - loss: 0.0394 - val_accuracy: 0.6500 - val_loss: 2.2216\n",
            "Epoch 72/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0383 - val_accuracy: 0.6500 - val_loss: 2.3005\n",
            "Epoch 73/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9880 - loss: 0.0151 - val_accuracy: 0.7000 - val_loss: 2.4786\n",
            "Epoch 74/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0147 - val_accuracy: 0.7000 - val_loss: 2.3562\n",
            "Epoch 75/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0114 - val_accuracy: 0.6500 - val_loss: 2.2558\n",
            "Epoch 76/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0132 - val_accuracy: 0.6500 - val_loss: 2.3016\n",
            "Epoch 77/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.6500 - val_loss: 2.3019\n",
            "Epoch 78/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0067 - val_accuracy: 0.6500 - val_loss: 2.3125\n",
            "Epoch 79/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0136 - val_accuracy: 0.6500 - val_loss: 2.3335\n",
            "Epoch 80/80\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9715 - loss: 0.0361 - val_accuracy: 0.6500 - val_loss: 2.3325\n",
            "\n",
            "----------------------------------------\n",
            "Test set evaluation:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7923\n",
            "Test error: 0.2077\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.54      0.64        13\n",
            "           1       0.91      0.77      0.83        13\n",
            "           2       1.00      0.92      0.96        13\n",
            "           3       0.77      0.77      0.77        13\n",
            "           4       0.71      0.77      0.74        13\n",
            "           5       0.87      1.00      0.93        13\n",
            "           6       0.60      0.69      0.64        13\n",
            "           7       0.80      0.62      0.70        13\n",
            "           8       0.87      1.00      0.93        13\n",
            "           9       0.69      0.85      0.76        13\n",
            "\n",
            "    accuracy                           0.79       130\n",
            "   macro avg       0.80      0.79      0.79       130\n",
            "weighted avg       0.80      0.79      0.79       130\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Top 5 parameter combinations:\n",
            "\n",
            "1. CV Score: 0.9077 (+/- 0.0760)\n",
            "   Time: 69.1s\n",
            "   Parameters:\n",
            "     filters: 24\n",
            "     dropout_rate: 0.4\n",
            "     dense_units: 128\n",
            "     learning_rate: 0.005\n",
            "     optimizer: adam\n",
            "     epochs: 80\n",
            "     batch_size: 16\n",
            "\n",
            "2. CV Score: 0.8846 (+/- 0.0025)\n",
            "   Time: 77.2s\n",
            "   Parameters:\n",
            "     filters: 24\n",
            "     dropout_rate: 0.3\n",
            "     dense_units: 128\n",
            "     learning_rate: 0.001\n",
            "     optimizer: adam\n",
            "     epochs: 80\n",
            "     batch_size: 8\n",
            "\n",
            "3. CV Score: 0.8770 (+/- 0.0424)\n",
            "   Time: 65.5s\n",
            "   Parameters:\n",
            "     filters: 32\n",
            "     dropout_rate: 0.2\n",
            "     dense_units: 64\n",
            "     learning_rate: 0.001\n",
            "     optimizer: rmsprop\n",
            "     epochs: 80\n",
            "     batch_size: 32\n",
            "\n",
            "4. CV Score: 0.8691 (+/- 0.1165)\n",
            "   Time: 60.4s\n",
            "   Parameters:\n",
            "     filters: 16\n",
            "     dropout_rate: 0.2\n",
            "     dense_units: 128\n",
            "     learning_rate: 0.01\n",
            "     optimizer: adam\n",
            "     epochs: 80\n",
            "     batch_size: 16\n",
            "\n",
            "5. CV Score: 0.8388 (+/- 0.0721)\n",
            "   Time: 53.0s\n",
            "   Parameters:\n",
            "     filters: 16\n",
            "     dropout_rate: 0.2\n",
            "     dense_units: 32\n",
            "     learning_rate: 0.001\n",
            "     optimizer: adam\n",
            "     epochs: 80\n",
            "     batch_size: 32\n",
            "\n",
            "Model saved as 'best_small_dataset_model.h5'\n",
            "Parameters saved to 'best_parameters_small.txt'\n",
            "\n",
            "Parameter search completed!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('sign_mnist_13bal_train.csv')\n",
        "test_data = pd.read_csv('sign_mnist_13bal_test.csv')\n",
        "\n",
        "X_train = train_data.drop('class', axis=1).values / 255.0\n",
        "y_train = train_data['class'].values\n",
        "X_test = test_data.drop('class', axis=1).values / 255.0\n",
        "y_test = test_data['class'].values\n",
        "\n",
        "y_train[y_train == 10] = 9\n",
        "y_test[y_test == 10] = 9\n",
        "\n",
        "print(\"Unique labels:\", np.unique(y_train))\n",
        "\n",
        "# Reshape as images for CNN input (28x28 grayscale)\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Define which parameters are for model building vs training\n",
        "MODEL_PARAMS = ['filters', 'dropout_rate', 'dense_units', 'learning_rate', 'optimizer']\n",
        "TRAINING_PARAMS = ['epochs', 'batch_size']\n",
        "\n",
        "def build_and_compile_model(filters=32, dropout_rate=0.3, dense_units=64,\n",
        "                           learning_rate=0.001, optimizer='adam'):\n",
        "    \"\"\"\n",
        "    Build and compile CNN model - ONLY accepts model architecture parameters\n",
        "    \"\"\"\n",
        "    print(f\"    Building model: filters={filters}, dropout={dropout_rate}, units={dense_units}, lr={learning_rate}, opt={optimizer}\")\n",
        "\n",
        "    model = Sequential([\n",
        "        tf.keras.Input(shape=(28, 28, 1)),\n",
        "        Conv2D(filters, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(filters * 2, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(dense_units, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Configure optimizer\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model_cv(param_dict, X_train, y_train, cv_folds=3):\n",
        "    \"\"\"\n",
        "    Evaluate model using cross-validation\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    cv_scores = []\n",
        "\n",
        "    # Extract model parameters (only the ones the function accepts)\n",
        "    model_params = {\n",
        "        'filters': param_dict['filters'],\n",
        "        'dropout_rate': param_dict['dropout_rate'],\n",
        "        'dense_units': param_dict['dense_units'],\n",
        "        'learning_rate': param_dict['learning_rate'],\n",
        "        'optimizer': param_dict['optimizer']\n",
        "    }\n",
        "\n",
        "    # Extract training parameters\n",
        "    epochs = param_dict['epochs']\n",
        "    batch_size = param_dict['batch_size']\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
        "        print(f\"    Fold {fold + 1}/{cv_folds}\", end=\" \")\n",
        "\n",
        "        # Split data\n",
        "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        # Build model with ONLY model parameters\n",
        "        model = build_and_compile_model(**model_params)\n",
        "\n",
        "        # Train model with training parameters\n",
        "        history = model.fit(\n",
        "            X_train_fold, y_train_fold,\n",
        "            epochs=epochs,\n",
        "            batch_size=min(batch_size, len(X_train_fold)),\n",
        "            validation_data=(X_val_fold, y_val_fold),\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Get best validation accuracy\n",
        "        best_val_acc = max(history.history['val_accuracy'])\n",
        "        cv_scores.append(best_val_acc)\n",
        "        print(f\"Score: {best_val_acc:.3f}\")\n",
        "\n",
        "        # Clear memory\n",
        "        del model\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    return np.mean(cv_scores), np.std(cv_scores)\n",
        "\n",
        "# Define parameter ranges optimized for small dataset\n",
        "param_ranges = {\n",
        "    'filters': [16, 24, 32],\n",
        "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
        "    'dense_units': [32, 64, 128],\n",
        "    'learning_rate': [0.001, 0.005, 0.01],\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'epochs': [30, 50, 80],\n",
        "    'batch_size': [8, 16, 32]\n",
        "}\n",
        "\n",
        "def generate_random_params(param_ranges, n_combinations=12):\n",
        "    \"\"\"\n",
        "    Generate random parameter combinations\n",
        "    \"\"\"\n",
        "    combinations = []\n",
        "\n",
        "    for _ in range(n_combinations):\n",
        "        params = {}\n",
        "        for param, values in param_ranges.items():\n",
        "            params[param] = random.choice(values)\n",
        "        combinations.append(params)\n",
        "\n",
        "    return combinations\n",
        "\n",
        "# Perform randomized search\n",
        "print(\"\\nGenerating random parameter combinations...\")\n",
        "param_combinations = generate_random_params(param_ranges, n_combinations=12)\n",
        "\n",
        "print(\"Starting Randomized Parameter Search...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = []\n",
        "best_score = 0\n",
        "best_params = None\n",
        "\n",
        "for i, params in enumerate(param_combinations):\n",
        "    print(f\"\\nCombination {i+1}/{len(param_combinations)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Print current parameters\n",
        "    print(\"Parameters:\")\n",
        "    for param, value in params.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Evaluate model\n",
        "        mean_score, std_score = evaluate_model_cv(params, X_train, y_train, cv_folds=3)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print(f\"  CV Result: {mean_score:.4f} (+/- {std_score*2:.4f})\")\n",
        "        print(f\"  Time: {elapsed_time:.1f}s\")\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'params': params.copy(),\n",
        "            'mean_score': mean_score,\n",
        "            'std_score': std_score,\n",
        "            'time': elapsed_time\n",
        "        })\n",
        "\n",
        "        # Update best parameters\n",
        "        if mean_score > best_score:\n",
        "            best_score = mean_score\n",
        "            best_params = params.copy()\n",
        "            print(\"  *** NEW BEST SCORE! ***\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "# Print final results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if best_params is not None:\n",
        "    print(f\"Best CV score: {best_score:.4f}\")\n",
        "    print(\"Best parameters:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    # Train final model\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"Training final model on full training set...\")\n",
        "\n",
        "    # Extract model parameters for final model\n",
        "    final_model_params = {\n",
        "        'filters': best_params['filters'],\n",
        "        'dropout_rate': best_params['dropout_rate'],\n",
        "        'dense_units': best_params['dense_units'],\n",
        "        'learning_rate': best_params['learning_rate'],\n",
        "        'optimizer': best_params['optimizer']\n",
        "    }\n",
        "\n",
        "    final_model = build_and_compile_model(**final_model_params)\n",
        "\n",
        "    # Train with best parameters\n",
        "    history = final_model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=best_params['epochs'],\n",
        "        batch_size=min(best_params['batch_size'], len(X_train)),\n",
        "        validation_split=0.15,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"Test set evaluation:\")\n",
        "\n",
        "    test_predictions = final_model.predict(X_test, verbose=0)\n",
        "    test_pred_classes = np.argmax(test_predictions, axis=1)\n",
        "    test_accuracy = accuracy_score(y_test, test_pred_classes)\n",
        "\n",
        "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test error: {1 - test_accuracy:.4f}\")\n",
        "\n",
        "    # Show classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, test_pred_classes))\n",
        "\n",
        "    # Show top results\n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    print(\"Top 5 parameter combinations:\")\n",
        "\n",
        "    if results:\n",
        "        results_sorted = sorted(results, key=lambda x: x['mean_score'], reverse=True)\n",
        "\n",
        "        for i, result in enumerate(results_sorted[:5]):\n",
        "            print(f\"\\n{i+1}. CV Score: {result['mean_score']:.4f} (+/- {result['std_score']*2:.4f})\")\n",
        "            print(f\"   Time: {result['time']:.1f}s\")\n",
        "            print(\"   Parameters:\")\n",
        "            for param, value in result['params'].items():\n",
        "                print(f\"     {param}: {value}\")\n",
        "\n",
        "    # Save results\n",
        "    try:\n",
        "        final_model.save('best_small_dataset_model.h5')\n",
        "\n",
        "        with open('best_parameters_small.txt', 'w') as f:\n",
        "            f.write(\"Best Parameters for Small Dataset:\\n\")\n",
        "            f.write(\"=\"*50 + \"\\n\")\n",
        "            for param, value in best_params.items():\n",
        "                f.write(f\"{param}: {value}\\n\")\n",
        "            f.write(f\"\\nCV Score: {best_score:.4f}\\n\")\n",
        "            f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "            f.write(f\"Dataset size: {len(X_train)} training samples\\n\")\n",
        "\n",
        "        print(f\"\\nModel saved as 'best_small_dataset_model.h5'\")\n",
        "        print(\"Parameters saved to 'best_parameters_small.txt'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't save files: {str(e)}\")\n",
        "\n",
        "else:\n",
        "    print(\"No successful parameter combinations found!\")\n",
        "    print(\"Check your data and try reducing parameter ranges.\")\n",
        "\n",
        "print(\"\\nParameter search completed!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQfuBaMe+RNx1/njU6RCaR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}